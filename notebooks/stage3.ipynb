{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851c32b0-5e4f-43da-81b8-86c4c72f6afd",
   "metadata": {},
   "source": [
    "## Script testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a48dfd-7d58-4f95-b76e-2847b930f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import FeatureHasher, Imputer, OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import DataFrame, SparkSession, Window\n",
    "from pyspark.sql.functions import count, countDistinct, lit, row_number, udf, when\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.param.shared import HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import udf, radians, sin, cos, lit, sqrt, pow\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, ArrayType\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofweek, hour, lit, cos, sin, udf, minute, second\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCols\n",
    "from pyspark.ml.util import DefaultParamsWritable, DefaultParamsReadable\n",
    "\n",
    "team = 13\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"{} - spark ML\".format(team))\n",
    "    .master(\"yarn\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse)\n",
    "    .config(\"spark.sql.avro.compression.codec\", \"snappy\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "def get_data() -> DataFrame:\n",
    "    global spark\n",
    "    df = spark.sql(\"SELECT * FROM team13_projectdb.accidents_partitioned_bucketed\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_numerical_features(df: DataFrame, numerical_cols: list) -> DataFrame:\n",
    "    global spark\n",
    "    window_state = Window.partitionBy(\"State\")\n",
    "\n",
    "    for column in numerical_cols:\n",
    "        df = df.withColumn(column, F.coalesce(F.col(column), F.mean(F.col(column)).over(window_state)))\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"num_features_raw\")\n",
    "\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=assembler.getOutputCol(), outputCol=\"scaled_num_features\", withMean=True, withStd=True\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    processed_df = pipeline_model.transform(df)\n",
    "\n",
    "    array_df = processed_df.withColumn(\"scaled_array\", vector_to_array(\"scaled_num_features\"))\n",
    "\n",
    "    for i, column in enumerate(numerical_cols):\n",
    "        array_df = array_df.withColumn(f\"scaled_{column}\", F.col(\"scaled_array\")[i].cast(\"double\"))\n",
    "\n",
    "    cols_to_drop = numerical_cols + [\"num_features_raw\", \"scaled_num_features\", \"scaled_array\"]\n",
    "    final_df = array_df.drop(*cols_to_drop)\n",
    "\n",
    "    for column in numerical_cols:\n",
    "        final_df = final_df.withColumnRenamed(f\"scaled_{column}\", column)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Helper to convert vector to array\n",
    "vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "class DropNullTimestampTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, colsToDropNulls=None):\n",
    "        super(DropNullTimestampTransformer, self).__init__()\n",
    "        self.colsToDropNulls = colsToDropNulls\n",
    "\n",
    "    def _transform(self, df):\n",
    "        if self.colsToDropNulls:\n",
    "            # Drop rows where any of the specified columns have null values\n",
    "            return df.na.drop(subset=self.colsToDropNulls)\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "\n",
    "class ExtractTimeFeaturesTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, timestampCols=None):\n",
    "        super(ExtractTimeFeaturesTransformer, self).__init__()\n",
    "        self.timestampCols = timestampCols\n",
    "\n",
    "    def _transform(self, df):\n",
    "        for ts_col in self.timestampCols:\n",
    "            df = df.withColumn(f\"{ts_col}_year\", year(ts_col).cast(DoubleType())) \\\n",
    "                   .withColumn(f\"{ts_col}_month\", month(ts_col).cast(DoubleType())) \\\n",
    "                   .withColumn(f\"{ts_col}_weekday\", dayofweek(ts_col).cast(DoubleType())) \\\n",
    "                   .withColumn(f\"{ts_col}_hour\", hour(ts_col).cast(DoubleType())) \\\n",
    "                   .withColumn(f\"{ts_col}_minute\", minute(ts_col).cast(DoubleType())) \\\n",
    "                   .withColumn(f\"{ts_col}_second\", second(ts_col).cast(DoubleType()))\n",
    "        return df\n",
    "\n",
    "\n",
    "class CyclicalEncodingTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, timestampCols=None):\n",
    "        super(CyclicalEncodingTransformer, self).__init__()\n",
    "        self.timestampCols = timestampCols\n",
    "\n",
    "    def _transform(self, df):\n",
    "        def cyclical_encode(col_name, period):\n",
    "            radians = (2 * lit(3.141592653589793) * col_name) / period\n",
    "            return cos(radians).cast(DoubleType()), sin(radians).cast(DoubleType())\n",
    "\n",
    "        for ts_col in self.timestampCols:\n",
    "            month_sin, month_cos = cyclical_encode(df[f\"{ts_col}_month\"], 12)\n",
    "            hour_sin, hour_cos = cyclical_encode(df[f\"{ts_col}_hour\"], 24)\n",
    "            minute_sin, minute_cos = cyclical_encode(df[f\"{ts_col}_minute\"], 60)\n",
    "            second_sin, second_cos = cyclical_encode(df[f\"{ts_col}_second\"], 60)\n",
    "\n",
    "            df = df.withColumn(f\"{ts_col}_month_sin\", month_sin) \\\n",
    "                   .withColumn(f\"{ts_col}_month_cos\", month_cos) \\\n",
    "                   .withColumn(f\"{ts_col}_hour_sin\", hour_sin) \\\n",
    "                   .withColumn(f\"{ts_col}_hour_cos\", hour_cos) \\\n",
    "                   .withColumn(f\"{ts_col}_minute_sin\", minute_sin) \\\n",
    "                   .withColumn(f\"{ts_col}_minute_cos\", minute_cos) \\\n",
    "                   .withColumn(f\"{ts_col}_second_sin\", second_sin) \\\n",
    "                   .withColumn(f\"{ts_col}_second_cos\", second_cos)\n",
    "        return df\n",
    "\n",
    "def encode_timestamp_features(df: DataFrame) -> DataFrame:\n",
    "    timestamp_cols = ['start_time', 'end_time', 'weather_timestamp']\n",
    "\n",
    "    # Step 1: Drop rows with nulls in weather_timestamp\n",
    "    drop_null_transformer = DropNullTimestampTransformer(colsToDropNulls=['weather_timestamp'])\n",
    "\n",
    "    # Step 2: Extract time features\n",
    "    extract_transformer = ExtractTimeFeaturesTransformer(timestampCols=timestamp_cols)\n",
    "\n",
    "    # Step 3: Apply cyclical encoding\n",
    "    cyclical_transformer = CyclicalEncodingTransformer(timestampCols=timestamp_cols)\n",
    "\n",
    "    # Step 4: Define columns to scale\n",
    "    columns_to_scale = []\n",
    "    for ts_col in timestamp_cols:\n",
    "        columns_to_scale.extend([\n",
    "            f\"{ts_col}_year\", \n",
    "            f\"{ts_col}_weekday\"\n",
    "        ])\n",
    "\n",
    "    # Step 5: Impute and scale\n",
    "    imputer = Imputer(\n",
    "        inputCols=columns_to_scale,\n",
    "        outputCols=[f\"{col}_imputed\" for col in columns_to_scale],\n",
    "        strategy=\"mean\"\n",
    "    )\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputer.getOutputCols(),\n",
    "        outputCol=\"features_to_scale\"\n",
    "    )\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_to_scale\",\n",
    "        outputCol=\"scaled_features\",\n",
    "        withMean=True,\n",
    "        withStd=True\n",
    "    )\n",
    "\n",
    "    # Step 6: Build pipeline\n",
    "    pipeline = Pipeline(stages=[\n",
    "        drop_null_transformer,\n",
    "        extract_transformer,\n",
    "        cyclical_transformer,\n",
    "        imputer,\n",
    "        assembler,\n",
    "        scaler\n",
    "    ])\n",
    "\n",
    "    # Step 7: Transform data\n",
    "    scaled_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "    # Step 8: Convert vector to array and extract scaled values\n",
    "    scaled_df = scaled_df.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "\n",
    "    for i, col in enumerate(columns_to_scale):\n",
    "        scaled_df = scaled_df.withColumn(f\"{col}_scaled\", scaled_df[\"scaled_array\"][i].cast(DoubleType()))\n",
    "\n",
    "    # Step 9: Clean up — now including minute and second\n",
    "    cols_to_drop = (\n",
    "        timestamp_cols +\n",
    "        [f\"{ts_col}_month\" for ts_col in timestamp_cols] +\n",
    "        [f\"{ts_col}_hour\" for ts_col in timestamp_cols] +\n",
    "        [f\"{ts_col}_minute\" for ts_col in timestamp_cols] +\n",
    "        [f\"{ts_col}_second\" for ts_col in timestamp_cols] +\n",
    "        columns_to_scale +\n",
    "        imputer.getOutputCols() +\n",
    "        [\"features_to_scale\", \"scaled_features\", \"scaled_array\"]\n",
    "    )\n",
    "\n",
    "    scaled_df = scaled_df.drop(*cols_to_drop)\n",
    "\n",
    "    # Step 10: Rename scaled columns back to original names\n",
    "    for col in columns_to_scale:\n",
    "        scaled_df = scaled_df.withColumnRenamed(f\"{col}_scaled\", col)\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "def impute_categorical_features(df: DataFrame, categorical_features: list) -> DataFrame:\n",
    "    # Identify categorical features with null values and impute them with mode values\n",
    "    df_imputed = df\n",
    "    features_with_nulls = []\n",
    "\n",
    "    # First identify which features have nulls\n",
    "    for feature in categorical_features:\n",
    "        missing_count = df.filter(df[feature].isNull()).count()\n",
    "        if missing_count > 0:\n",
    "            features_with_nulls.append(feature)\n",
    "            print(f\"Feature {feature} has {missing_count} missing values - will impute with mode\")\n",
    "\n",
    "    # Special handling for City - use mode by State\n",
    "    if \"City\" in features_with_nulls:\n",
    "        print(\"\\nSpecial handling for City: Using mode city within each State\")\n",
    "\n",
    "        # Calculate mode city for each state\n",
    "        state_city_modes = (\n",
    "            df.filter(F.col(\"City\").isNotNull())\n",
    "            .groupBy(\"State\", \"City\")\n",
    "            .count()\n",
    "            .orderBy(\"State\", F.col(\"count\").desc())\n",
    "        )\n",
    "\n",
    "        # Get the most common city in each state\n",
    "        window = Window.partitionBy(\"State\").orderBy(F.col(\"count\").desc())\n",
    "        city_modes_by_state = (\n",
    "            state_city_modes.withColumn(\"row\", row_number().over(window))\n",
    "            .filter(F.col(\"row\") == 1)\n",
    "            .select(\"State\", \"City\")\n",
    "        )\n",
    "\n",
    "        # Get the global mode for fallback\n",
    "        global_city_mode = (\n",
    "            df.filter(F.col(\"City\").isNotNull())\n",
    "            .groupBy(\"City\")\n",
    "            .count()\n",
    "            .orderBy(F.col(\"count\").desc())\n",
    "            .limit(1)\n",
    "            .collect()[0][\"City\"]\n",
    "        )\n",
    "\n",
    "        print(f\"Global city mode (fallback): {global_city_mode}\")\n",
    "\n",
    "        # Collect the state-city mapping to a dictionary for faster lookup\n",
    "        state_to_city_dict = {row[\"State\"]: row[\"City\"] for row in city_modes_by_state.collect()}\n",
    "\n",
    "        # Create a UDF to map state to its mode city\n",
    "        def get_mode_city_by_state(state):\n",
    "            return state_to_city_dict.get(state, global_city_mode)\n",
    "\n",
    "        mode_city_udf = udf(get_mode_city_by_state, StringType())\n",
    "\n",
    "        # Apply imputation for City based on State\n",
    "        df_imputed = df_imputed.withColumn(\n",
    "            \"City\", when(F.col(\"City\").isNull(), mode_city_udf(F.col(\"State\"))).otherwise(F.col(\"City\"))\n",
    "        )\n",
    "\n",
    "        # Remove City from features_with_nulls as it's handled separately\n",
    "        features_with_nulls.remove(\"City\")\n",
    "\n",
    "    # Calculate modes and apply imputation for other features with nulls\n",
    "    for feature in features_with_nulls:\n",
    "        # Calculate mode value (most frequent non-null value)\n",
    "        mode_value = (\n",
    "            df.filter(F.col(feature).isNotNull())\n",
    "            .groupBy(feature)\n",
    "            .count()\n",
    "            .orderBy(\"count\", ascending=False)\n",
    "            .limit(1)\n",
    "            .collect()[0][feature]\n",
    "        )\n",
    "\n",
    "        print(f\"Imputing {feature} with mode value: {mode_value}\")\n",
    "\n",
    "        # Apply imputation\n",
    "        df_imputed = df_imputed.withColumn(\n",
    "            feature, when(F.col(feature).isNull(), lit(mode_value)).otherwise(F.col(feature))\n",
    "        )\n",
    "\n",
    "    # Set working dataframe to imputed version\n",
    "    df = df_imputed\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_categorical_features(df: DataFrame) -> DataFrame:\n",
    "    # 1. Frequency encoding for high-cardinality features (County and City)\n",
    "    # Calculate frequency counts\n",
    "    county_freq = df.groupBy(\"County\").count()\n",
    "    city_freq = df.groupBy(\"City\").count()\n",
    "\n",
    "    # Calculate total counts for percentage calculation\n",
    "    total_count = df.count()\n",
    "    county_freq = county_freq.withColumn(\"county_frequency\", F.col(\"count\") / total_count)\n",
    "    city_freq = city_freq.withColumn(\"city_frequency\", F.col(\"count\") / total_count)\n",
    "\n",
    "    # Join frequency information back to the main dataframe\n",
    "    df = df.join(county_freq.select(\"County\", \"county_frequency\"), on=\"County\", how=\"left\")\n",
    "    df = df.join(city_freq.select(\"City\", \"city_frequency\"), on=\"City\", how=\"left\")\n",
    "\n",
    "    # 2. For low/medium cardinality features, use StringIndexer + OneHotEncoder\n",
    "    side_indexer = StringIndexer(inputCol=\"side\", outputCol=\"SideIndex\", handleInvalid=\"keep\")\n",
    "    side_encoder = OneHotEncoder(inputCol=\"SideIndex\", outputCol=\"side_vec\")\n",
    "\n",
    "    state_indexer = StringIndexer(inputCol=\"state\", outputCol=\"StateIndex\", handleInvalid=\"keep\")\n",
    "    state_encoder = OneHotEncoder(inputCol=\"StateIndex\", outputCol=\"state_vec\")\n",
    "\n",
    "    weather_indexer = StringIndexer(inputCol=\"Weather_Condition\", outputCol=\"WeatherIndex\", handleInvalid=\"keep\")\n",
    "    weather_encoder = OneHotEncoder(inputCol=\"WeatherIndex\", outputCol=\"weather_vec\")\n",
    "\n",
    "    # Create pipeline for the encoders (excluding the frequency encoding part)\n",
    "    pipeline = Pipeline(\n",
    "        stages=[\n",
    "            side_indexer,\n",
    "            side_encoder,\n",
    "            state_indexer,\n",
    "            state_encoder,\n",
    "            weather_indexer,\n",
    "            weather_encoder,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply pipeline transformation\n",
    "    model = pipeline.fit(df)\n",
    "    df = model.transform(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "class GeoToECEFTransformer(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    inputInRadians = Param(\n",
    "        Params._dummy(),\n",
    "        \"inputInRadians\",\n",
    "        \"Whether input coordinates are in radians (True) or degrees (False)\",\n",
    "        typeConverter=TypeConverters.toBoolean,\n",
    "    )\n",
    "\n",
    "    def __init__(self, inputCols=None, outputCols=None, inputInRadians=False):\n",
    "        super(GeoToECEFTransformer, self).__init__()\n",
    "        self._setDefault(inputCols=None, outputCols=None, inputInRadians=False)\n",
    "\n",
    "        if inputCols and outputCols:\n",
    "            self.setInputCols(inputCols)\n",
    "            self.setOutputCols(outputCols)\n",
    "\n",
    "        self.setInputInRadians(inputInRadians)\n",
    "\n",
    "    def setInputCols(self, value):\n",
    "        \"\"\"\n",
    "        Sets the input columns.\n",
    "        Expected order: [latitude, longitude, altitude]\n",
    "        \"\"\"\n",
    "        return self._set(inputCols=value)\n",
    "\n",
    "    def setOutputCols(self, value):\n",
    "        \"\"\"\n",
    "        Sets the output columns.\n",
    "        Expected order: [ecef_x, ecef_y, ecef_z]\n",
    "        \"\"\"\n",
    "        return self._set(outputCols=value)\n",
    "\n",
    "    def setInputInRadians(self, value):\n",
    "        \"\"\"\n",
    "        Sets whether input coordinates are in radians (True) or degrees (False).\n",
    "        \"\"\"\n",
    "        return self._set(inputInRadians=value)\n",
    "\n",
    "    def getInputInRadians(self):\n",
    "        \"\"\"\n",
    "        Gets whether input coordinates are in radians.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.inputInRadians)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Transform the dataset by converting geographic coordinates to ECEF coordinates.\n",
    "        \"\"\"\n",
    "        # Constants from the Java class\n",
    "        a = 6378137.0  # WGS-84 semi-major axis\n",
    "        e2 = 6.6943799901377997e-3  # WGS-84 first eccentricity squared\n",
    "\n",
    "        # Get input column names\n",
    "        lat_col, lon_col, alt_col = self.getInputCols()\n",
    "\n",
    "        # Get output column names\n",
    "        x_col, y_col, z_col = self.getOutputCols()\n",
    "\n",
    "        # Create a combined UDF for ECEF calculation\n",
    "        def geo_to_ecef(lat, lon, alt):\n",
    "            \"\"\"\n",
    "            Convert geographic coordinates to ECEF.\n",
    "            Input: latitude and longitude in radians, altitude in meters\n",
    "            Output: tuple of (x, y, z) in meters\n",
    "            \"\"\"\n",
    "            if lat is None or lon is None or alt is None:\n",
    "                return (None, None, None)\n",
    "\n",
    "            # Calculate the radius of curvature in the prime vertical\n",
    "            n = a / math.sqrt(1 - e2 * math.sin(lat) * math.sin(lat))\n",
    "\n",
    "            # Calculate ECEF coordinates\n",
    "            x = (n + alt) * math.cos(lat) * math.cos(lon)\n",
    "            y = (n + alt) * math.cos(lat) * math.sin(lon)\n",
    "            z = (n * (1 - e2) + alt) * math.sin(lat)\n",
    "\n",
    "            return (x, y, z)\n",
    "\n",
    "        # Register UDF with appropriate return type\n",
    "        ecef_schema = StructType(\n",
    "            [\n",
    "                StructField(\"x\", DoubleType(), True),\n",
    "                StructField(\"y\", DoubleType(), True),\n",
    "                StructField(\"z\", DoubleType(), True),\n",
    "            ]\n",
    "        )\n",
    "        ecef_udf = udf(geo_to_ecef, ecef_schema)\n",
    "\n",
    "        # If input is in degrees, convert to radians first\n",
    "        if not self.getInputInRadians():\n",
    "            lat_expr = radians(F.col(lat_col))\n",
    "            lon_expr = radians(F.col(lon_col))\n",
    "        else:\n",
    "            lat_expr = F.col(lat_col)\n",
    "            lon_expr = F.col(lon_col)\n",
    "\n",
    "        # Apply the transformation\n",
    "        result = dataset.withColumn(\"ecef_coords\", ecef_udf(lat_expr, lon_expr, F.col(alt_col)))\n",
    "        # Extract individual ECEF components\n",
    "        result = result.withColumn(x_col, result.ecef_coords.x)\n",
    "        result = result.withColumn(y_col, result.ecef_coords.y)\n",
    "        result = result.withColumn(z_col, result.ecef_coords.z)\n",
    "\n",
    "        # Drop the temporary struct column\n",
    "        return result.drop(\"ecef_coords\")\n",
    "\n",
    "    def copy(self, extra=None):\n",
    "        \"\"\"\n",
    "        Creates a copy of this instance.\n",
    "        \"\"\"\n",
    "        if extra is None:\n",
    "            extra = {}\n",
    "        return super(GeoToECEFTransformer, self).copy(extra)\n",
    "\n",
    "\n",
    "def encode_spatial_features(df: DataFrame) -> DataFrame:\n",
    "    if \"Altitude\" not in df.columns:\n",
    "        df = df.withColumn(\"Altitude\", lit(0.0))\n",
    "\n",
    "    geo_to_ecef_start = GeoToECEFTransformer(\n",
    "        inputCols=[\"Start_Lat\", \"Start_Lng\", \"Altitude\"],\n",
    "        outputCols=[\"ecef_start_x\", \"ecef_start_y\", \"ecef_start_z\"],\n",
    "        inputInRadians=False,\n",
    "    )\n",
    "    geo_to_ecef_end = GeoToECEFTransformer(\n",
    "        inputCols=[\"End_Lat\", \"End_Lng\", \"Altitude\"],\n",
    "        outputCols=[\"ecef_end_x\", \"ecef_end_y\", \"ecef_end_z\"],\n",
    "        inputInRadians=False,\n",
    "    )\n",
    "    pipeline = Pipeline(stages=[geo_to_ecef_start, geo_to_ecef_end])\n",
    "    df = pipeline.fit(df).transform(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleanup_data(df: DataFrame) -> DataFrame:\n",
    "    df = df.drop(\n",
    "        \"City\",\n",
    "        \"county\",\n",
    "        \"start_lat\",\n",
    "        \"start_lng\",\n",
    "        \"end_lat\",\n",
    "        \"end_lng\",\n",
    "        \"side\",\n",
    "        \"Weather_Condition\",\n",
    "        \"state\",\n",
    "        \"sunrise_sunset\",\n",
    "        \"SideIndex\",\n",
    "        \"WeatherIndex\",\n",
    "        \"StateIndex\",\n",
    "        \"Altitude\",\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdcb83-2425-43fc-8ae2-3679c376bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f381d9cd-3c33-4d07-8fed-1b00bd486c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"distance_mi\",\n",
    "    \"temperature_f\",\n",
    "    \"wind_chill_f\",\n",
    "    \"humidity_percent\",\n",
    "    \"pressure_in\",\n",
    "    \"visibility_mi\",\n",
    "    \"wind_speed_mph\",\n",
    "    \"precipitation_in\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"Side\", \"City\", \"County\", \"Weather_Condition\", \"State\"]\n",
    "\n",
    "encoded_df = preprocess_numerical_features(df, numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa62979f-38ab-4ae3-b1cb-35008c03c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = encode_timestamp_features(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90433e63-618b-4b40-bb9c-af29eaf2d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature City has 136 missing values - will impute with mode\n",
      "Feature Weather_Condition has 19900 missing values - will impute with mode\n",
      "\n",
      "Special handling for City: Using mode city within each State\n",
      "Global city mode (fallback): Miami\n",
      "Imputing Weather_Condition with mode value: Fair\n"
     ]
    }
   ],
   "source": [
    "encoded_df = impute_categorical_features(encoded_df, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c426e482-4a65-49d1-8e61-09399c440afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = encode_categorical_features(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b48c4b54-f3f8-42b8-98ba-32048ef6ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = encode_spatial_features(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e84a27b7-7079-4a95-a999-7d1403512fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = cleanup_data(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "99d3d01e-905e-48cb-8ffb-d002d1faa2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------\n",
      " id                           | A-828658             \n",
      " severity                     | 2                    \n",
      " distance_mi                  | -0.4057452258518801  \n",
      " temperature_f                | 1.151593692195921    \n",
      " wind_chill_f                 | 1.1882032202674286   \n",
      " humidity_percent             | -0.5014226742221052  \n",
      " pressure_in                  | 0.5661143189473108   \n",
      " visibility_mi                | 0.33571104611777897  \n",
      " wind_speed_mph               | -0.815321075545157   \n",
      " precipitation_in             | -0.08368500835329849 \n",
      " start_time_month_sin         | 0.5000000000000001   \n",
      " start_time_month_cos         | -0.8660254037844386  \n",
      " start_time_hour_sin          | -1.0                 \n",
      " start_time_hour_cos          | 1.224646799147353... \n",
      " start_time_minute_sin        | -0.9945218953682734  \n",
      " start_time_minute_cos        | -0.10452846326765305 \n",
      " start_time_second_sin        | 1.0                  \n",
      " start_time_second_cos        | 0.0                  \n",
      " end_time_month_sin           | 0.5000000000000001   \n",
      " end_time_month_cos           | -0.8660254037844386  \n",
      " end_time_hour_sin            | -0.9659258262890683  \n",
      " end_time_hour_cos            | -0.2588190451025208  \n",
      " end_time_minute_sin          | 0.30901699437494723  \n",
      " end_time_minute_cos          | -0.9510565162951536  \n",
      " end_time_second_sin          | 0.6691306063588585   \n",
      " end_time_second_cos          | -0.743144825477394   \n",
      " weather_timestamp_month_sin  | 0.5000000000000001   \n",
      " weather_timestamp_month_cos  | -0.8660254037844386  \n",
      " weather_timestamp_hour_sin   | -1.0                 \n",
      " weather_timestamp_hour_cos   | 1.224646799147353... \n",
      " weather_timestamp_minute_sin | -0.8660254037844388  \n",
      " weather_timestamp_minute_cos | -0.4999999999999997  \n",
      " weather_timestamp_second_sin | 1.0                  \n",
      " weather_timestamp_second_cos | 0.0                  \n",
      " start_time_year              | 0.7101519402308575   \n",
      " start_time_weekday           | 1.024536752404059    \n",
      " end_time_year                | 0.7095949798821876   \n",
      " end_time_weekday             | 1.0100933375214558   \n",
      " weather_timestamp_year       | 0.7101519402308575   \n",
      " weather_timestamp_weekday    | 1.024536752404059    \n",
      " county_frequency             | 4.651818538999773... \n",
      " city_frequency               | 9.87259026853875E-4  \n",
      " side_vec                     | (2,[0],[1.0])        \n",
      " state_vec                    | (49,[15],[1.0])      \n",
      " weather_vec                  | (127,[0],[1.0])      \n",
      " ecef_start_x                 | -116440.52853003275  \n",
      " ecef_start_y                 | -5479959.994025315   \n",
      " ecef_start_z                 | 3250523.464079404    \n",
      " ecef_end_x                   | -116331.9207365441   \n",
      " ecef_end_y                   | -5479968.837870713   \n",
      " ecef_end_z                   | 3250512.5169962086   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_df.limit(1).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63449ba-fc30-4b65-8b74-278b85a2f4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split data and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f1cdcbf0-f70f-402f-b081-e756d82bdbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "def prepare_and_save_data(\n",
    "    encoded_df: DataFrame,\n",
    "    test_size: float = 0.3,\n",
    "    label_col: str = \"severity\",\n",
    "    feature_exclude_cols: list = [\"id\", \"severity\"],\n",
    "    hdfs_train_path: str = \"/user/team13/project/data/train\",\n",
    "    hdfs_test_path: str = \"/user/team13/project/data/test\",\n",
    "    local_train_path: str = \"/home/team13/project/BigData-Car-Accident/data/train.json\",\n",
    "    local_test_path: str = \"/home/team13/project/BigData-Car-Accident/data/test.json\"\n",
    "):\n",
    "\n",
    "    feature_cols = [col for col in encoded_df.columns if col not in feature_exclude_cols]\n",
    "    train_df, test_df = encoded_df.randomSplit([1 - test_size, test_size], seed=42)\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "\n",
    "    train_assembled = assembler.transform(train_df).select(\"features\", F.col(label_col).alias(\"label\"))\n",
    "    test_assembled = assembler.transform(test_df).select(\"features\", F.col(label_col).alias(\"label\"))\n",
    "\n",
    "    train_assembled.select(\"features\", \"label\") \\\n",
    "        .coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"json\") \\\n",
    "        .save(hdfs_train_path)\n",
    "\n",
    "    test_assembled.select(\"features\", \"label\") \\\n",
    "        .coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"json\") \\\n",
    "        .save(hdfs_test_path)\n",
    "\n",
    "    run(f\"hdfs dfs -cat {hdfs_train_path}/*.json > {local_train_path}\")\n",
    "    run(f\"hdfs dfs -cat {hdfs_test_path}/*.json > {local_test_path}\")\n",
    "\n",
    "    print(f\"Train data saved to: {local_train_path}\")\n",
    "    print(f\"Test data saved to: {local_test_path}\")\n",
    "\n",
    "    return train_assembled, test_assembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4964105-0ff2-43ee-888d-0e412c6e5081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to: /home/team13/project/BigData-Car-Accident/data/train.json\n",
      "Test data saved to: /home/team13/project/BigData-Car-Accident/data/test.json\n"
     ]
    }
   ],
   "source": [
    "train, test = prepare_and_save_data(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc4a15-4971-44ca-9020-edd2d7dbcdec",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bc4c9-4b32-4b07-b37b-4a362c1d7d0f",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90c7b008-0518-499a-87a1-cfbd2a7a1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def train_classifier(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    modelType=\"lr\"\n",
    "):\n",
    "    if modelType == \"dt\":\n",
    "        classifier = DecisionTreeClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            seed=42\n",
    "        )\n",
    "    elif modelType == \"lr\":\n",
    "        classifier = LogisticRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            family=\"multinomial\",\n",
    "            regParam=0.01,\n",
    "            elasticNetParam=0.0\n",
    "        )\n",
    "        \n",
    "    print(\"Training model...\")\n",
    "    model = classifier.fit(train_df)\n",
    "    \n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    test_acc = evaluator.evaluate(predictions)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "    test_f1 = evaluator_f1.evaluate(predictions)\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    for i in [1, 2, 3, 4]:\n",
    "        print(f\"Severity: {i}\")\n",
    "        evaluator_pbl = MulticlassClassificationEvaluator(metricName=\"precisionByLabel\", metricLabel=i)\n",
    "        test_pbl = evaluator_pbl.evaluate(predictions)\n",
    "        print(f\"Test PBL: {test_pbl}\")\n",
    "\n",
    "        evaluator_rbl = MulticlassClassificationEvaluator(metricName=\"recallByLabel\", metricLabel=i)\n",
    "        test_rbl = evaluator_rbl.evaluate(predictions)\n",
    "        print(f\"Test RBL: {test_rbl}\")\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cbe85150-0b48-4c45-a6ed-1d005fc6ed9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Test Accuracy: 0.8912\n",
      "Test F1 Score: 0.8588\n",
      "Severity: 1\n",
      "Test PBL: 0.6404494382022472\n",
      "Test RBL: 0.6404494382022472\n",
      "Severity: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib64/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-7fbec7a19c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-2bc57bfbac8f>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(train_df, test_df, modelType)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Severity: {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mevaluator_pbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"precisionByLabel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetricLabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtest_pbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator_pbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test PBL: {test_pbl}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_classifier(train, test, \"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "be231bdc-6408-4bb0-b56d-85371687100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(\n",
    "    predictions: DataFrame,\n",
    "    hdfs_path: str,\n",
    "    local_path: str\n",
    "):\n",
    "    # Select only the necessary columns and coalesce to one partition\n",
    "    selected_df = predictions.select(\"label\", \"prediction\").coalesce(1)\n",
    "\n",
    "    # Write to HDFS as CSV with header\n",
    "    selected_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .save(hdfs_path)\n",
    "\n",
    "    # Copy from HDFS to local filesystem\n",
    "    run(f\"hdfs dfs -cat {hdfs_path}/*.csv > {local_path}\")\n",
    "\n",
    "    print(f\"Predictions saved to HDFS at: {hdfs_path}\")\n",
    "    print(f\"Local CSV file saved to: {local_path}\")\n",
    "\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a8423c69-cc99-4db4-bbb1-61d57647d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def save_metrics_to_csv(\n",
    "    metrics_list,\n",
    "    local_file_path: str,\n",
    "    hdfs_file_path: str\n",
    "):\n",
    "\n",
    "    # Create DataFrame\n",
    "    global spark\n",
    "    metrics_df = spark.createDataFrame(metrics_list)\n",
    "    # Save to CSV\n",
    "    metrics_df.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .save(hdfs_file_path)\n",
    "\n",
    "    run(f\"hdfs dfs -cat {hdfs_file_path}/*.csv > {local_file_path}\")\n",
    "\n",
    "    print(f\"Metrics saved successfully to: {local_file_path}\")\n",
    "    return local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a071c501-ed06-4963-befd-76e5c338df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def train_lr_with_grid_search(\n",
    "    train_df: DataFrame,\n",
    "    test_df: DataFrame,\n",
    "    output_model_path: str = \"project/models/model1\",\n",
    "    output_prediction_path: str = \"project/output/model1_predictions\"\n",
    "):\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        family=\"multinomial\"\n",
    "    )\n",
    "\n",
    "    # paramGrid = ParamGridBuilder() \\\n",
    "    #     .addGrid(lr.regParam, [0.1]) \\ #[0.01, 0.2])\n",
    "    #     .addGrid(lr.elasticNetParam, [0.0]) \\ #[0.0, 0.8]) \n",
    "    #     .build()\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.1])  \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0]) \\\n",
    "        .build()\n",
    "\n",
    "    # 5. Define evaluator (F1 score used for model selection)\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "    # 6. Set up CrossValidator\n",
    "    crossval = CrossValidator(\n",
    "        estimator=lr,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=2,\n",
    "        # numFolds=3,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # 7. Train the CrossValidator on the training data\n",
    "    print(\"Training cross validator...\")\n",
    "    cv_model = crossval.fit(train_df)\n",
    "    \n",
    "    # 8. Extract the best model from cross-validation\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # 9. Save the best model to HDFS\n",
    "    best_model.write().overwrite().save(output_model_path)\n",
    "    \n",
    "    # 10. Make predictions on test data\n",
    "    predictions = best_model.transform(test_df)\n",
    "\n",
    "    # 11. Evaluate on test data using both accuracy and F1\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    test_acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    test_f1 = evaluator.evaluate(predictions)\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    model_data = {\n",
    "        \"f1\": test_f1,\n",
    "        \"acc\": test_acc,\n",
    "    }\n",
    "    \n",
    "    for i in [1, 2, 3, 4]:\n",
    "        try:\n",
    "            print(f\"Severity: {i}\")\n",
    "            evaluator_pbl = MulticlassClassificationEvaluator(metricName=\"precisionByLabel\", metricLabel=i)\n",
    "            test_pbl = evaluator_pbl.evaluate(predictions)\n",
    "            print(f\"Test PBL: {test_pbl}\")\n",
    "\n",
    "            evaluator_rbl = MulticlassClassificationEvaluator(metricName=\"recallByLabel\", metricLabel=i)\n",
    "            test_rbl = evaluator_rbl.evaluate(predictions)\n",
    "            print(f\"Test RBL: {test_pbl}\")\n",
    "\n",
    "            model_data[f\"recall_{i}\"] = test_rbl\n",
    "            model_data[f\"precision_{i}\"] = test_pbl\n",
    "        except Exception:\n",
    "            model_data[f\"recall_{i}\"] = -1\n",
    "            model_data[f\"precision_{i}\"] = -1\n",
    "\n",
    "    model_data[\"name\"] = f\"lr: regParam:{best_model.getRegParam()}, elasticNetParam: {best_model.getElasticNetParam()}\"\n",
    "\n",
    "    return predictions, model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e4f5b321-d36b-4de3-a78f-35a49f28cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cross validator...\n",
      "Test Accuracy: 1.0000\n",
      "Test F1 Score: 1.0000\n",
      "Severity: 1\n",
      "Severity: 2\n",
      "Test PBL: 1.0\n",
      "Test RBL: 1.0\n",
      "Severity: 3\n",
      "Severity: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 1.0,\n",
       " 'acc': 1.0,\n",
       " 'recall_1': -1,\n",
       " 'precision_1': -1,\n",
       " 'recall_2': 1.0,\n",
       " 'precision_2': 1.0,\n",
       " 'recall_3': -1,\n",
       " 'precision_3': -1,\n",
       " 'recall_4': -1,\n",
       " 'precision_4': -1,\n",
       " 'name': 'lr: regParam:0.1, elasticNetParam: 0.0'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1, model_data1 = train_lr_with_grid_search(train, test)\n",
    "model_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d0984-591e-4ffe-93ce-1c06ae0477b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_to_csv(\n",
    "    metrics_list=[model_data1],\n",
    "    local_file_path=\"/home/team13/project/BigData-Car-Accident/output/evaluation.csv\",\n",
    "    hdfs_file_path=\"/user/team13/project/output/evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "70a5b183-b4f1-4112-a787-45bc3f164362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to HDFS at: /user/team13/project/output/model1_predictions\n",
      "Local CSV file saved to: /home/team13/project/BigData-Car-Accident/output/model1_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/team13/project/BigData-Car-Accident/output/model1_predictions.csv'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_predictions(\n",
    "    predictions1,\n",
    "    hdfs_path=\"/user/team13/project/output/model1_predictions\",\n",
    "    local_path=\"/home/team13/project/BigData-Car-Accident/output/model1_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166493d4-14c3-4c7e-b482-e257d746f5a4",
   "metadata": {},
   "source": [
    "# Second Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88735ce-da55-4c12-94e2-ae0fad5c0c28",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621afb06-7945-4d50-84d1-67fd449a08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier(train, test, \"dt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ebf91-d7c0-476e-9e77-f8f8bd93b70c",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "92a07167-9654-47da-ae22-c969ec143ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def train_dt_with_grid_search(\n",
    "    train_df: DataFrame,\n",
    "    test_df: DataFrame,\n",
    "    output_model_path: str = \"project/models/model1\",\n",
    "    output_prediction_path: str = \"project/output/model1_predictions\"\n",
    "):\n",
    "    dt = DecisionTreeClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # paramGrid = ParamGridBuilder() \\\n",
    "    #     .addGrid(dt.maxDepth, [3, 5]) \\\n",
    "    #     .addGrid(dt.impurity, [\"gini\", \"entropy\"]) \\\n",
    "    #     .build()\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(dt.maxDepth, [2]) \\\n",
    "        .addGrid(dt.impurity, [\"gini\"]) \\\n",
    "        .build()\n",
    "\n",
    "    # 5. Define evaluator (F1 score used for model selection)\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "    # 6. Set up CrossValidator\n",
    "    crossval = CrossValidator(\n",
    "        estimator=dt,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=2,\n",
    "        # numFolds=3,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # 7. Train the CrossValidator on the training data\n",
    "    print(\"Training cross validator...\")\n",
    "    cv_model = crossval.fit(train_df)\n",
    "    \n",
    "    # 8. Extract the best model from cross-validation\n",
    "    best_model = cv_model.bestModel\n",
    "\n",
    "    # 9. Save the best model to HDFS\n",
    "    best_model.write().overwrite().save(output_model_path)\n",
    "\n",
    "    # 10. Make predictions on test data\n",
    "    predictions = best_model.transform(test_df)\n",
    "\n",
    "    # 11. Evaluate on test data using both accuracy and F1\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    test_acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    test_f1 = evaluator.evaluate(predictions)\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    model_data = {\n",
    "        \"f1\": test_f1,\n",
    "        \"acc\": test_acc,\n",
    "    }\n",
    "    \n",
    "    for i in [1, 2, 3, 4]:\n",
    "        try:\n",
    "            print(f\"Severity: {i}\")\n",
    "            evaluator_pbl = MulticlassClassificationEvaluator(metricName=\"precisionByLabel\", metricLabel=i)\n",
    "            test_pbl = evaluator_pbl.evaluate(predictions)\n",
    "            print(f\"Test PBL: {test_pbl}\")\n",
    "\n",
    "            evaluator_rbl = MulticlassClassificationEvaluator(metricName=\"recallByLabel\", metricLabel=i)\n",
    "            test_rbl = evaluator_rbl.evaluate(predictions)\n",
    "            print(f\"Test RBL: {test_pbl}\")\n",
    "\n",
    "            model_data[f\"recall_{i}\"] = test_rbl\n",
    "            model_data[f\"precision_{i}\"] = test_pbl\n",
    "        except Exception:\n",
    "            model_data[f\"recall_{i}\"] = -1\n",
    "            model_data[f\"precision_{i}\"] = -1\n",
    "        \n",
    "    model_data[\"name\"] = f\"dt: maxDepth:{best_model.getMaxDepth()}, impurity: {best_model.getImpurity()}\"\n",
    "\n",
    "    return predictions, model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "92454b72-2ed7-4089-a358-3ba811e9dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cross validator...\n",
      "Test Accuracy: 1.0000\n",
      "Test F1 Score: 1.0000\n",
      "Severity: 1\n",
      "Severity: 2\n",
      "Test PBL: 1.0\n",
      "Test RBL: 1.0\n",
      "Severity: 3\n",
      "Severity: 4\n"
     ]
    }
   ],
   "source": [
    "predictions2, model_data2 = train_dt_with_grid_search(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "406c9950-fc9d-49d5-a215-2cc664d99647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+\n",
      "|acc| f1|                name|precision_1|precision_2|precision_3|precision_4|recall_1|recall_2|recall_3|recall_4|\n",
      "+---+---+--------------------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+\n",
      "|1.0|1.0|lr: regParam:0.1,...|         -1|        1.0|         -1|         -1|      -1|     1.0|      -1|      -1|\n",
      "|1.0|1.0|lr: maxDepth:2, i...|         -1|        1.0|         -1|         -1|      -1|     1.0|      -1|      -1|\n",
      "+---+---+--------------------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+\n",
      "\n",
      "Metrics saved successfully to: /home/team13/project/BigData-Car-Accident/output/evaluation.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/team13/project/BigData-Car-Accident/output/evaluation.csv'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_metrics_to_csv(\n",
    "    metrics_list=[model_data1, model_data2],\n",
    "    local_file_path=\"/home/team13/project/BigData-Car-Accident/output/evaluation.csv\",\n",
    "    hdfs_file_path=\"/user/team13/project/output/evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "88bd5aa5-4680-4e6f-8878-0faeed414ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to HDFS at: /user/team13/project/output/model2_predictions\n",
      "Local CSV file saved to: /home/team13/project/BigData-Car-Accident/output/model2_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/team13/project/BigData-Car-Accident/output/model2_predictions.csv'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_predictions(\n",
    "    predictions2,\n",
    "    hdfs_path=\"/user/team13/project/output/model2_predictions\",\n",
    "    local_path=\"/home/team13/project/BigData-Car-Accident/output/model2_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbad32-7529-41a9-880f-3b75477ea57f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
